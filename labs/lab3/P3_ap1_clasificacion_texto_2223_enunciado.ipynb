{"cells":[{"cell_type":"markdown","metadata":{"id":"OUBRwgEu3yu1"},"source":["# Práctica 2: Procesamiento del Lenguaje Natural\n","\n","__Fecha de entrega: 8 de mayo de 2023__\n","\n","El objetivo de esta práctica es aplicar los conceptos teóricos vistos en clase en el módulo de PLN. La práctica consta de 2 notebooks que se entregarán simultáneamente en la tarea de entrega habilitada en el Campus  Virtual.\n","\n","Lo más importante en esta práctica no es el código Python, sino el análisis de los datos y modelos que construyas y las explicaciones razonadas de cada una de las decisiones que tomes. __No se valorarán trozos de código o gráficas sin ningún tipo de contexto o explicación__.\n","\n","Finalmente, recuerda establecer el parámetro `random_state` en todas las funciones que tomen decisiones aleatorias para que los resultados sean reproducibles (los resultados no varíen entre ejecuciones)."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"V3YxCTUW3yu9"},"outputs":[],"source":["RANDOM_STATE = 1234"]},{"cell_type":"markdown","metadata":{"id":"pn_YQLVL3yvA"},"source":["# Apartado 1: Análisis de sentimientos\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"de-i8w0s3yvC"},"source":["__Número de grupo: 20__\n","\n","__Nombres de los estudiantes: Alejandro Barrachina Argudo y Juan Pablo Corella Martín__"]},{"cell_type":"markdown","metadata":{"id":"yeVD_g2D3yvC"},"source":["## 1) Carga del conjunto de datos\n","\n","El fichero `IMBD_Dataset.csv` contiene opiniones de películas clasificadas en 2 categorías diferentes (positiva/negativa).\n","\n","Este set de datos se creó utilizando el \"IMDB Dataset of 50K Movie Reviews\", el cual contiene 50,000 reseñas de películas con un sentimiento positivo o negativo adjunto a ellas.\n","\n","Muestra un ejemplo de cada clase.\n","\n","Haz un estudio del conjunto de datos. ¿qué palabras aparecen más veces?, ¿tendría sentido normalizar de alguna manera el corpus?\n","\n","Crea una partición de los datos dejando el 80% para entrenamiento y el 20% restante para test usando la función `train_test_split` de sklearn. Comprueba que la distribución de los ejemplos en las clases es la misma en entrenamiento y test. "]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1487,"status":"ok","timestamp":1681627835928,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"},"user_tz":-120},"id":"5YyPy4BDfzGQ","outputId":"d3637bed-fd90-4b68-9372-ed77292ae301"},"outputs":[],"source":["# acceso a google drive\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"0csu2B8N3yvE"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":920,"status":"ok","timestamp":1681627836845,"user":{"displayName":"ALBERTO DIAZ ESTEBAN","userId":"09370147929418307454"},"user_tz":-120},"id":"kmcEfPgYf1Fk","outputId":"e7aed290-7b25-478a-8bed-3872fd756c12"},"outputs":[],"source":["# imbd_file = '/content/drive/MyDrive/IA2/p3/IMDB_Dataset.csv'\n","\n","# df=pd.read_csv(imbd_file)\n","# df.head()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["imbd_file = './IMDB_Dataset.csv'\n","\n","df = pd.read_csv(imbd_file)\n","df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"r4rXv3xX3yvG"},"source":["## 2) Estudio del efecto de distintas representaciones y distintos algoritmos para resolver la tarea\n","\n","Construye distintas representaciones vectoriales basadas en lo contado en las clases de teoría (bolsas de palabras con 2 configuraciones distintas significativas) y utilízalas con 2 de los algoritmos estudiados (árboles de decisión y naive bayes)\n","\n","Para una única configuración, muestra algún mensaje tanto en su formato de texto original como en la versión vectorizada. ¿Qué palabras se han eliminado y por qué?\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to c:\\JP\\UNIVERSIDAD\\AÑO\n","[nltk_data]     6\\IA 2\\Prácticas\\labs\\lab3\\data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["from os import path, getcwd\n","\n","nltk_path = path.join(getcwd(), 'data')\n","nltk.data.path.append(nltk_path)\n","nltk.download('stopwords', nltk_path)\n","\n","wpt = nltk.WordPunctTokenizer()\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","\n","def normalize_document(doc):\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = wpt.tokenize(doc)\n","    # filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","\n","normalize_corpus = np.vectorize(normalize_document)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","df.review = normalize_corpus(df.review)\n","train_data, test_data = train_test_split(df, test_size=0.2,random_state=RANDOM_STATE)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["154794\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(stop_words='english')\n","# Tomamos los textos del conjunto de entrenamiento y los transformamos en \n","# una matriz de datos (palabras) según el diccionario estándar\n","train_vector_data=vectorizer.fit_transform(train_data.review)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","print(len(feature_names))\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['00' '02' '03' '10' '100' '11' '12' '13' '14' '14th' '15' '150'\n"," '150_worst_cases_of_nepotism' '16' '17' '18' '1850' '1887' '1888' '1890'\n"," '19' '1915' '1920' '1922' '1923' '1933' '1942' '1945' '1949' '1950'\n"," '1955' '1958' '1968' '1970' '1970s' '1971' '1974' '1975' '1976' '1977'\n"," '1978' '1980' '1987' '1988' '1990' '1991' '1992' '1996' '1997' '1999'\n"," '19th' '1s' '1st' '20' '2002' '2004' '2005' '2008' '2009' '2012' '2040'\n"," '20th' '21849889' '21849890' '21st' '25' '26' '27' '28' '2nd' '2s' '2x'\n"," '30' '30s' '320x180' '35' '35yr' '3d' '3rd' '40' '400' '40s' '44yrs' '45'\n"," '45ish' '50' '52' '54' '5th' '60' '60s' '70' '700' '70s' '72' '75' '78'\n"," '7th' '80' '90']\n","['zorros' 'zorrostyle' 'zors' 'zosch' 'zoschs' 'zosh' 'zoundsbr' 'zouzou'\n"," 'zoweebr' 'zozo' 'zp' 'zplan' 'zrated' 'zs' 'zsazsa' 'zschering'\n"," 'zseries' 'zshops' 'zshornack' 'zsigmond' 'zsrr' 'zsrs' 'zu' 'zubeidaa'\n"," 'zuber' 'zubr' 'zucchiniheaded' 'zucco' 'zuccoleaves' 'zuccon'\n"," 'zuccopreparing' 'zuccos' 'zucker' 'zuckerabrams' 'zuckerman'\n"," 'zuckermanfill' 'zuckers' 'zuckert' 'zucovic' 'zudina' 'zues' 'zuger'\n"," 'zugsmith' 'zugurt' 'zukhov' 'zukicks' 'zukor' 'zukovics' 'zula'\n"," 'zulaaynurmzeyyan' 'zulabr' 'zuleika' 'zuleta' 'zulu' 'zuluagain'\n"," 'zulubr' 'zulus' 'zumhofe' 'zungia' 'zuni' 'zuniga' 'zunigas' 'zunz'\n"," 'zuotian' 'zurer' 'zurers' 'zurich' 'zurlini' 'zurlinis' 'zurn' 'zutaut'\n"," 'zuthe' 'zuwarriors' 'zuzzzuzz' 'zvezda' 'zvonimir' 'zvyagvatsev' 'zwart'\n"," 'zwartboek' 'zwarts' 'zweite' 'zwick' 'zwicks' 'zwrite' 'zx' 'zy' 'zyada'\n"," 'zylbersteinand' 'zyuranger' 'zz' 'zzvorkov' 'zzzz' 'zzzzip' 'zzzzzs'\n"," 'zzzzzzzz' 'zzzzzzzzz' 'zzzzzzzzzzzz' 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'\n"," 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'\n"," 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz']\n"]}],"source":["print(feature_names[:100])\n","print(feature_names[-100:])"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\JP\\UNIVERSIDAD\\AÑO 6\\IA 2\\Prácticas\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1380: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["['2' '1080' '&c' '10-point' '10th' '11-point' '12-point' '16-point'\n"," '18-point' '1st' '2,4,5-t' '2,4-d' '20-point' '2D' '2nd' '30-30' '3D'\n"," '3-D' '3M' '3rd' '48-point' '4-D' '4GL' '4H' '4th' '5-point' '5-T' '5th'\n"," '6-point' '6th' '7-point' '7th' '8-point' '8th' '9-point' '9th' 'a' \"a'\"\n"," 'a-' 'A&M' 'A&P' 'A.' 'A.A.A.' 'A.B.' 'A.B.A.' 'A.C.' 'A.D.' 'A.D.C.'\n"," 'A.F.' 'A.F.A.M.' 'A.G.' 'A.H.' 'A.I.' 'A.I.A.' 'A.I.D.' 'A.L.' 'A.L.P.'\n"," 'A.M.' 'A.M.A.' 'A.M.D.G.' 'A.N.' 'a.p.' 'a.r.' 'A.R.C.S.' 'A.U.'\n"," 'A.U.C.' 'A.V.' 'a.w.' 'A.W.O.L.' 'A/C' 'A/F' 'A/O' 'A/P' 'A/V' 'A1'\n"," 'A-1' 'A4' 'A5' 'AA' 'AAA' 'AAAA' 'AAAAAA' 'AAAL' 'AAAS' 'Aaberg'\n"," 'Aachen' 'AAE' 'AAEE' 'AAF' 'AAG' 'aah' 'aahed' 'aahing' 'aahs' 'AAII'\n"," 'aal' 'Aalborg' 'Aalesund' 'aalii' 'aaliis']\n"]}],"source":["#* Mucho ruido, vamos a usar nuestro vocabulario\n","with open('./words.txt') as f:\n","    dictionary = f.read().splitlines()\n","\n","# El diccionario cargado lo pasamos en el parámetro vocabulary\n","vectorizer = CountVectorizer(vocabulary=dictionary, lowercase=True, stop_words='english')\n","train_vector_data=vectorizer.fit_transform(train_data.review)\n","feature_names = vectorizer.get_feature_names_out()\n","print(feature_names[:100])"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['Zsigmondy' 'Zsolway' 'ZST' 'ZT' 'Ztopek' 'Zubeneschamali' 'Zubird'\n"," 'Zubkoff' 'zubr' 'Zuccari' 'zuccarino' 'Zuccaro' 'Zucchero' 'zucchetti'\n"," 'zucchetto' 'zucchettos' 'zucchini' 'zucchinis' 'zucco' 'zuchetto'\n"," 'Zucker' 'Zuckerman' 'zudda' 'zuffolo' 'zufolo' 'Zug' 'zugtierlast'\n"," 'zugtierlaster' 'zugzwang' 'Zui' 'Zuian' 'Zuidholland' 'zuisin' 'Zulch'\n"," 'Zuleika' 'Zulema' 'Zulhijjah' 'Zulinde' 'Zulkadah' \"Zu'lkadah\"\n"," 'Zullinger' 'Zullo' 'Zuloaga' 'Zulu' 'Zuludom' 'Zuluize' 'Zulu-kaffir'\n"," 'Zululand' 'Zulus' 'zumatic' 'zumbooruk' 'Zumbrota' 'Zumstein' 'Zumwalt'\n"," 'Zungaria' 'Zuni' 'Zunian' 'zunyite' 'zunis' 'zupanate' 'Zupus' 'Zurbar'\n"," 'Zurbaran' 'Zurek' 'Zurheide' 'Zurich' 'Zurkow' 'zurlite' 'Zurn' 'Zurvan'\n"," 'Zusman' 'Zutugil' 'zuurveldt' 'zuza' 'Zuzana' 'Zu-zu' 'zwanziger'\n"," 'Zwart' 'ZWEI' 'Zweig' 'Zwick' 'Zwickau' 'Zwicky' 'Zwieback' 'zwiebacks'\n"," 'Zwiebel' 'zwieselite' 'Zwingle' 'Zwingli' 'Zwinglian' 'Zwinglianism'\n"," 'Zwinglianist' 'zwitter' 'zwitterion' 'zwitterionic' 'Zwolle' 'Zworykin'\n"," 'ZZ' 'zZt' 'ZZZ']\n"]}],"source":["print(feature_names[-100:])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 461456)\t0.12579327195123483\n","  (0, 454391)\t0.11654297540482404\n","  (0, 453100)\t0.1495545210419253\n","  (0, 445295)\t0.09268354866400144\n","  (0, 445293)\t0.09475761101154959\n","  (0, 426779)\t0.1608778895495493\n","  (0, 423590)\t0.1608778895495493\n","  (0, 422359)\t0.21746189786580866\n","  (0, 394746)\t0.07214431807474121\n","  (0, 383473)\t0.16688079779319984\n","  (0, 374782)\t0.11901079647355263\n","  (0, 342778)\t0.09206920838064399\n","  (0, 342771)\t0.07156688015720021\n","  (0, 341007)\t0.06925475631966892\n","  (0, 323928)\t0.10152542915658123\n","  (0, 323795)\t0.10164086656900866\n","  (0, 317643)\t0.11383341421971684\n","  (0, 309133)\t0.09626776607595595\n","  (0, 299724)\t0.07699852501227651\n","  (0, 299619)\t0.08270757288905833\n","  (0, 289704)\t0.051560922570476064\n","  (0, 285822)\t0.10662618048602177\n","  (0, 254206)\t0.15952308533682372\n","  (0, 252607)\t0.1502236279159648\n","  (0, 246767)\t0.032235194819850516\n","  :\t:\n","  (0, 146633)\t0.1023708912286036\n","  (0, 145031)\t0.14143745344924072\n","  (0, 143770)\t0.10010057023782518\n","  (0, 138922)\t0.03439176312814564\n","  (0, 137919)\t0.15045141779271165\n","  (0, 133566)\t0.09061357749733868\n","  (0, 131076)\t0.18360999274388806\n","  (0, 123301)\t0.07643111533375817\n","  (0, 118851)\t0.11948704825286773\n","  (0, 105498)\t0.07642402894192671\n","  (0, 105264)\t0.06315804278872257\n","  (0, 96844)\t0.20882624098987854\n","  (0, 88466)\t0.10518408276838721\n","  (0, 79639)\t0.09042132983074778\n","  (0, 79516)\t0.11730200986874352\n","  (0, 79509)\t0.07297756047710356\n","  (0, 65945)\t0.11543487194348385\n","  (0, 65652)\t0.18758241699818212\n","  (0, 61476)\t0.13275331201839863\n","  (0, 52536)\t0.11844281213487894\n","  (0, 44982)\t0.12453596691378267\n","  (0, 37586)\t0.15419756367033347\n","  (0, 34931)\t0.08541332361030007\n","  (0, 31234)\t0.16242752922391968\n","  (0, 28147)\t0.1137106579694928\n"]}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","\n","# Calculamos el valor TF-IDF \n","\n","tfidfer = TfidfTransformer()\n","train_preprocessed = tfidfer.fit_transform(train_vector_data)\n","\n","print(train_preprocessed[10])"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["import numpy as np\n","import numpy.ma as ma\n","\n","def write_terms (feature_names, data, vector_data, index):\n","    '''\n","    Escribe los términos presentes en un mensaje representado como bolsa de palabras.\n","    \n","    - feature_names: terminos usados para vectorizar\n","    - data: lista de mensajes original (si data==None no se muestra el mensaje original)\n","    - vector_data: matriz (dispersa) de mensaje vectorizados\n","    - index: posición del mensaje a mostrar\n","    '''\n","    # máscara para seleccionar sólo el mensaje en posición index\n","    mask=vector_data[index,:]>0\n","    \n","    # términos que aparecen en ese mensaje vectorizado\n","    terminos = ma.array(feature_names, mask = ~(mask[0].toarray()))\n","    \n","    # mostrar mensaje original\n","    if data is not None:\n","        print('Mensaje', index, ':', data[index])\n","    \n","    # mostrar términos que aparecen en el mensaje vectorizado\n","    print('Mensaje', index, 'vectorizado:', terminos.compressed(),'\\n')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mensaje 1000 vectorizado: ['actors' 'amateur' 'attacked' 'babes' 'bad' 'classified' 'considering'\n"," 'dead' 'film' 'flick' 'genre' 'interesting' 'living' 'make' 'mistake'\n"," 'presented' 'project' 'real' 'really' 'shooting' 'shots' 'sight'\n"," 'staying' 'sure' 'thought' 'titling' 'twist' 'used' 'violence' 'women'\n"," 'zombie'] \n","\n"]}],"source":["write_terms(feature_names, None, train_vector_data, 1000)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["\n","# Tomamos los textos del conjunto de test y los transformamos en una matriz\n","# de palabras. Al usar \"transform\" toma como referencia únicamente las palabras\n","# encontradas en el conjunto de entrenamiento\n","test_vector_data=vectorizer.transform(test_data.review)\n","# Calculamos el valor TF-IDF \n","# Al usar \"transform\" toma como IDF el del conjunto de entrenamiento \n","test_preprocessed=tfidfer.transform(test_vector_data)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["ENTRENAMOS LOS MODELOS"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["a.1) Árbol de decisión con TF-IDF"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Árbol, porcentaje de aciertos en entrenamiento: 1.0\n","Árbol, porcentaje de aciertos en test: 0.7196\n"]}],"source":["from sklearn import tree\n","import numpy as np\n","\n","# Creamos el clasificador con los valores por defecto\n","tree_classifier = tree.DecisionTreeClassifier()\n","tree_classifier.fit(train_preprocessed, train_data.sentiment)\n","\n","tree_train_predictions = tree_classifier.predict(train_preprocessed)\n","tree_test_predictions = tree_classifier.predict(test_preprocessed)\n","\n","print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == train_data.sentiment))\n","print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == test_data.sentiment))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["a.2) Árbol de decisión con CountVectorizer"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Árbol, porcentaje de aciertos en entrenamiento: 1.0\n","Árbol, porcentaje de aciertos en test: 0.7219\n"]}],"source":["# Creamos el clasificador con los valores por defecto\n","tree_classifier = tree.DecisionTreeClassifier()\n","tree_classifier.fit(train_vector_data, train_data.sentiment)\n","\n","tree_train_predictions = tree_classifier.predict(train_vector_data)\n","tree_test_predictions = tree_classifier.predict(test_vector_data)\n","\n","print(\"Árbol, porcentaje de aciertos en entrenamiento:\", np.mean(tree_train_predictions == train_data.sentiment))\n","print(\"Árbol, porcentaje de aciertos en test:\", np.mean(tree_test_predictions == test_data.sentiment))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["b.1) Naive Bayes con TF_IDF"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento: 0.8944\n","Multinomial Naive Bayes, porcentaje de aciertos en test: 0.8565\n"]}],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","mnb_classifier = MultinomialNB()\n","\n","mnb_classifier.fit(train_preprocessed, train_data.sentiment)\n","\n","mnb_train_predictions = mnb_classifier.predict(train_preprocessed)\n","mnb_test_predictions = mnb_classifier.predict(test_preprocessed)\n","\n","print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == train_data.sentiment))\n","print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == test_data.sentiment))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["b.2) Naive Bayes con Count-Vectorizer"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento: 0.882175\n","Multinomial Naive Bayes, porcentaje de aciertos en test: 0.8413\n"]}],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","mnb_classifier = MultinomialNB()\n","\n","mnb_classifier.fit(train_vector_data, train_data.sentiment)\n","\n","mnb_train_predictions = mnb_classifier.predict(train_vector_data)\n","mnb_test_predictions = mnb_classifier.predict(test_vector_data)\n","\n","print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento:\", np.mean(mnb_train_predictions == train_data.sentiment))\n","print(\"Multinomial Naive Bayes, porcentaje de aciertos en test:\", np.mean(mnb_test_predictions == test_data.sentiment))"]},{"cell_type":"markdown","metadata":{"id":"XmM1ftJe3yvK"},"source":["## 3) Análisis comparativo final\n","\n","Se han entrenado varios clasificadores usando vectorizaciones diferentes de los datos. Compara las diferencias entre representaciones para un mismo algoritmo y entre algoritmos. Explica a qué crees que se deben las diferencias.\n","\n","Analiza con detalle el mejor clasificador de cada tipo. Indica las palabras más relevantes. Busca un ejemplo mal clasificado de cada clase, justifica el error ¿se te ocurre alguna forma de solucionarlo?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qU0UFbSm9NQm"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"IA","language":"python","name":"ia"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
